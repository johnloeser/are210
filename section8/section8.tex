\documentclass[12pt,english]{article}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{verbatim}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{changepage}
\usepackage{enumitem}
\setlist{nolistsep}
\onehalfspacing
\usepackage{babel}
\newcommand{\expec}{\ensuremath{\mathbb E}}
\newcommand{\T}{\ensuremath{\text{T}}}
\begin{document}
\begin{center}
{\Large{}Section 8: Estimators and statistics}
\par\end{center}{\Large \par}

\begin{center}
ARE 210
\par\end{center}

\begin{center}
October 24, 2017
\par\end{center}

The section notes are available on the section Github at \href{github.com/johnloeser/are210}{github.com/johnloeser/are210} in the ``section8'' folder.

Quick note - I won't be around next week Monday and Tuesday. We need to pick a two hour slot next week - either Wednesday 10-12 or Thursday 8-10. We may be constrained to Thursday due to room availability.

\section{Definitions}

\begin{itemize}
\item \textbf{Estimation}
	\begin{itemize}
		\item \textbf{MLE}: $\{ Y_{i} \}_{i=1}^{n}$ iid, $Y_{i} \sim P_{\theta}$ with density $p_{\theta}$
		\begin{itemize}
			\item $\theta = \arg \max_{b} \mathbf{E}_{P_{\theta}} [\log p_{b}(Y)]$ (proof using KLIC + Jensen's inequality)
			\item $\hat{\theta}_{MLE} = \arg \max_{b} \frac{1}{n} \sum_{i=1}^{n} \log p_{b}(y_{i})$
			\item \textbf{Invariance property of MLE}: Let $P_{\theta_{0}} \in \{P_{\theta} : \theta \in \Theta\}$, and let $\lambda_{0} = h(\theta_{0})$. Then $\hat{\lambda}_{MLE} = h(\hat{\theta}_{MLE})$.
			\item \textbf{CMLE}: $\hat{\theta}_{CMLE} = \arg \max_{b \in \Theta} \frac{1}{n} \sum_{i=1}^{n} \log p_{b}(y_{i} | z_{i})$
		\end{itemize}
		\item \textbf{M-estimation}: Implicitly define $\theta(P) = \arg \min_{b \in \Theta} \mathbf{E}_{P} [q(X, b)]$, assume $\{ X_{i} \}_{i=1}^{n} \sim$ iid $P$. Then $\hat{\theta}_{M} = \arg \min_{b \in \Theta} \frac{1}{n} \sum_{i=1}^{n} q(X_{i}, b)$
		\begin{itemize}
			\item \textbf{NLLS}: $q(X_{i}, b) = (Y_{i} - \Phi(Z_{i}, b))^{2}$
			\item \textbf{Quantile}: $q_{Q}(X_{i}, b) = (X_{i} - b)(\mathbf{1}\{ Y > b \} - Q)$
		\end{itemize}
		\item \textbf{Method of moments}: $\{ X_{i} \}_{i=1}^{n}$ iid, implicitly define $\mathbf{E}_{P}[m(X, \theta(P))] = 0$
		\begin{itemize}
			\item Estimator: $\arg \min_{b} (\frac{1}{n} \sum_{i=1}^{n} m(x_{i}, b))^{\T} S (\frac{1}{n} \sum_{i=1}^{n} m(x_{i}, b))$
		\end{itemize}
		\item \textbf{Minimum distance}: $\hat{\theta}_{n} = \arg \min_{b} g_{n}(b)^{\T} S g_{n}(b)$
		\begin{itemize}
			\item ``Classical'' minimum distance: $g_{n}(b) = \Pi_{n} - h(b)$
		\end{itemize}
	\end{itemize}
	\item \textbf{Statistics}
	\begin{itemize}
		\item A function of the data $T(X)$ is a \textbf{statistic}
		\item $X \sim P_{\theta_{0}}$, $\theta_{0} \in \Theta$, $T(X)$ is \textbf{sufficient} for $\textbf{P} = \{ P_{\theta} : \theta \in \Theta \}$ (or sufficient for $\theta$) if $P_{\theta}[X | T] = P[X | T]$
		\item \textbf{Factorization theorem}: $\{ f_{\theta} : \theta \in \Theta \}$, $T(X)$ statistic, $T$ is sufficient $\Leftrightarrow$ $f(X, \theta) = g(T(X), \theta) h(X)$
		\item \textbf{Exponential family of distributions}: \\$p(x, \theta) = h(x) \exp( \sum_{j=1}^{K} \eta_{j}(\theta) T_{j}(x) - \beta(\theta))$
		\item $T$ is a \textbf{minimally sufficient} statistic if and only if $T$ is sufficient, and $T$ is a function of any other sufficient statistic $S$
		\item $S$ is \textbf{ancillary} for $\theta$ if the distribution of $S$ does not depend on $\theta$
		\item $T$ is \textbf{complete} for $\theta$ if $\forall$ $g$ such that $\mathbf{E}_{\theta}[g(T)] = 0$ $\forall$ $\theta \in \Theta$, $g(T) = 0$ almost everywhere.
		\item $X_{i}$ iid, $p(X, \theta)$ from the exponential family, then \\$T(X_{1}, \ldots, X_{n}) = \left( \sum_{i=1}^{n} T_{1}(X_{i}), \ldots, \sum_{i=1}^{n} T_{J}(X_{i}) \right)$ is complete for $\theta$ if \\$\{ \eta_{1}(\theta), \ldots, \eta_{J}(\theta) \}$ contains an open set in $\mathbf{R}^{J}$. By the factorization theorem, $T(X_{1}, \ldots, X_{n})$ is always sufficient for $\theta$.
		\item \textbf{Basu}: If $T$ is complete and sufficient, then $T \perp$ every ancillary statistic.
	\end{itemize}
\end{itemize}

\section{Some useful bits}
\begin{enumerate}
	\item It's useful to think about sufficient statistics as capturing weakly more information than complete statistics, and the complete statistic with the most information is minimally sufficient, while a minimally sufficient statistic is complete. Ancillary statistics are roughly the parts of the data that are orthogonal to our sufficient statistics. All of this may or may not be strictly true but the intuition roughly holds.
\end{enumerate}

\section{Practice questions}

1) (Midterm, Question 6) Let $Z=g(X)(Y-\expec(Y|X))$ for some measurable  function $g:\mathbb{R}\rightarrow\mathbb{R}$. Assume that all moments in
question exist.

a) Compute $\mathbf{E}(Z|X)$.

b) Compute $V(Z)$

c) Now let Let $Z=\mathbf{E}(g(X))(Y-\expec(Y|X))$. Compute $V(Z)$.

\vspace{1em}
\noindent
2) (Midterm, Question 7) Let $Y_{i}^{*}\sim N(\theta,1)$ and let
$Y_{i}=\mathbb{I}\{Y_{i}^{*}>0\}$. Suppose we observe a an i.i.d. sample
$\{Y_{i}\}_{i=1}^{n}$.

a) Let $\Phi^{-1}$ denote the inverse of the cumulative
distribution function (CDF) of the $N(0,1)$ distribution and
let $\phi$ denote the probability density function of the
$N(0,1)$ distribution. Find
the MLE for $\theta$ and denote it by $\hat{\theta}_{n}$.  

b) Show whether $\hat{\theta}_{n}$ converges in probability to  $\theta$. 

c) Derive the limiting distribution for (an appropriately
normalized) $\hat{\theta}_{n}$

\vspace{1em}
\noindent
3) (Problem Set 4, Question 4) (\textbf{Identification in an Endogenous Non-Parametric Regression Model})  Suppose that we have a model given by%
\begin{equation*}
y_{i}=\mu \left( x_{i}\right) +\epsilon _{i}
\end{equation*}%
and that we assume $\mathbb{E}\left( \epsilon _{i}|z_{i}\right) =0$
for some variable $z_{i}$ and the model is non-parametric in the sense
that we place no restrictions on the form of the (unknown) function $\mu
\left( \cdot \right) .$ \ Consider taking expectations conditional on
$z$ to obtain%
\begin{equation*}
\int yf\left( y|z\right) dy=\int \mu \left( x\right) f\left( x|z\right) dx
\end{equation*}%
The above is an example of an integral equation. \ Note that since we
observe $\left( y,x,z\right) $ the density functions $f\left( y|z\right) $
and $f\left( x|z\right) $ are identified. \ We will show that this integral
equation will have a unique solution if the distribution of $x$ conditional
on $z$ is complete (in $z$). 

Suppose that there exists another function $%
\tilde{\mu}\left( \cdot \right) $ that satisfies the equation, then we must
have%
\begin{equation*}
\int \left( \mu \left( x\right) -\tilde{\mu}\left( x\right) \right) f\left(
x|z\right) dx=0
\end{equation*}%
In order for their to be a unique solution therefore we must have the
following hold (setting $\delta \left( x\right) =\mu \left( x\right) -\tilde{%
	\mu}\left( x\right) $)%
\begin{equation}
\int \delta \left( x\right) f\left( x|z\right) dx=0\Rightarrow \delta \left(
x\right) =0  \label{id}
\end{equation}%
for almost all $z.$ \ Suppose that the distribution of $x$ conditional on $z$
is given by%
\begin{equation}
f\left( x|z\right) =h\left( x\right) \exp\left(\eta \left(
z\right) \tau \left( x\right)-B(z)\right)  \label{bing2}
\end{equation}%
Suppose further that the function $\tau \left( \cdot \right) $ is
one-to-one.\footnote{Formally we also need to assume that the support
	of $\mu \left( z\right) $ contains an open set and $%
	h\left( x\right) >0$.} \ Show that this implies (\ref{id}) so a
sufficient condition for the integral equation to have a unique
solution is the distributional assumption (\ref{bing2}%
) (Hint: Use the result on Completeness for Exponential Families. See
Newey \& Powell (2003) for an extension to cases where the
conditional distribution does not belong to an exponential family.)

\end{document}